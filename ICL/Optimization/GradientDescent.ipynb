{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11769b61",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization method used to find a local minimum for a differentiable function $f:X\\subseteq\\mathbb{R}^{n} \\rightarrow Y\\subseteq\\mathbb{R}$. The key idea is that the gradient of a function points in the direction of steepest ascent and thus by moving in the direction opposite to the gradient $-\\nabla f(\\boldsymbol x)$ the value of $f$ decreases the fastest.\n",
    "\n",
    "For an initial guess of the vector $\\boldsymbol{a}_{0}$ that minimizes $f$ and using an aggresion parameter $\\gamma\\in\\mathbb{R}$ that specifies the \"step size\", gradient descent is a method defined by the iterative rule:\n",
    "$$\\boldsymbol{a}_{n+1} = \\boldsymbol{a}_{n} - \\gamma\\nabla f\\left(\\boldsymbol{a}_{n}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce9e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(delF, a0, gamma, n):\n",
    "    a = a0\n",
    "    for i in range(n):\n",
    "        a = a - gamma*delF(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae895f",
   "metadata": {},
   "source": [
    "For gradient descent to work effectively $\\gamma$ must be \"sufficiently\" small. However if its too small it will take too long to converge, if its too big, well you might skip the minima. One way to determine an effective aggression parameter is to use the inverse of the Hessian matrix. This way, the iterative rule becomes:\n",
    "$$\\boldsymbol{a}_{n+1} = \\boldsymbol{a}_{n} - \\mathcal{H}^{-1}\\left(\\boldsymbol{a}_{n}\\right)\\nabla f\\left(\\boldsymbol{a}_{n}\\right),$$\n",
    "\n",
    "where $\\mathcal{H}$ is the Hessian matrix: $\\mathcal{H}_{ij}(\\boldsymbol{x}) = \\partial_{x_{i}x_{j}}^2 f$.\n",
    "\n",
    "However, the Hessian might change the direction of the gradient and thus find a maximum instead of a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0bec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessianDescent(delF, a0, H, n):\n",
    "    a = a0\n",
    "    for i in range(n):\n",
    "        a = a - np.linalg.inv(H(a)) @ delF(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca665e20",
   "metadata": {},
   "source": [
    "To get the best of both worlds we can use a method which tries the Hessian unless the step $- \\mathcal{H}^{-1}\\left(\\boldsymbol{a}_{n}\\right)\\nabla f\\left(\\boldsymbol{a}_{n}\\right)$ would be too big in magnitude or if it would point backwards, in which case it goes back to using steepest descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32f422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridDescent(delF, a0, H, n) :\n",
    "    gamma = 0.2\n",
    "    a = a0\n",
    "    step = lambda a : -linalg.inv(H(a)) @ delF(a)\n",
    "    for i in range(n):\n",
    "        if step @ - delF(a) <= 0 or linalg.norm(step) > 2 :\n",
    "            step = -gamma * delF(a)\n",
    "        a = a + step\n",
    "    return a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
